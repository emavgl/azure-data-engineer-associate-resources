- [DP-200](#dp-200)
  * [Implement Data Storage Solutions](#implement-data-storage-solutions)
    + [Storage Account creation](#storage-account-creation)
    + [Storage account tabs](#storage-account-tabs)
  * [CosmosDB](#cosmosdb)
    + [Create CosmosDB instance](#create-cosmosdb-instance)
    + [Access and Security](#access-and-security)
    + [Replication and failover](#replication-and-failover)
      - [Multi write conflict managment](#multi-write-conflict-managment)
      - [Failover](#failover)
      - [Backups](#backups)
    + [Consistency](#consistency)
    + [Database/Container/RU](#database-container-ru)
    + [Partitions](#partitions)
      - [Parititions and throughput](#parititions-and-throughput)
      - [CosmosDB Limitations and composite keys](#cosmosdb-limitations-and-composite-keys)
    + [Manage the data inside](#manage-the-data-inside)
      - [Insert/Query data](#insert-query-data)
      - [Time to Live](#time-to-live)
  * [Relational Database](#relational-database)
    + [Azure SQL Database](#azure-sql-database)
    + [Create a SQL Database](#create-a-sql-database)
    + [Pricing models](#pricing-models)
    + [Elastic Pool](#elastic-pool)
    + [Security](#security)
    + [High Availabilty](#high-availabilty)
      - [What about an availability zone goes down?](#what-about-an-availability-zone-goes-down-)
      - [What about a region goes down?](#what-about-a-region-goes-down-)
      - [Backups](#backups-1)
  * [Azure Synapse Analytics](#azure-synapse-analytics)
    + [Create new Azure Synapse Analytics - SQL Pool](#create-new-azure-synapse-analytics---sql-pool)
    + [Backups](#backups-2)
    + [What is MPP](#what-is-mpp)
      - [Sharding Patterns](#sharding-patterns)
      - [Table types](#table-types)
      - [Table partitioning](#table-partitioning)
    + [Loading Method](#loading-method)
    + [Set up Polybase](#set-up-polybase)
      - [Creates Master Key and securely store storage Credentials](#creates-master-key-and-securely-store-storage-credentials)
      - [Create an external data source](#create-an-external-data-source)
      - [Create external file format](#create-external-file-format)
      - [Create external table](#create-external-table)
      - [Load destination table](#load-destination-table)
  * [Securing Data Platform](#securing-data-platform)
    + [Key security components](#key-security-components)
    + [Azure Defender for Storage](#azure-defender-for-storage)
    + [Managing Authentication and Authorization](#managing-authentication-and-authorization)
    + [Monitoring Database accesses](#monitoring-database-accesses)
    + [Data Masking](#data-masking)
    + [Encrypt data at rest](#encrypt-data-at-rest)
  * [Stream Analytics](#stream-analytics)
    + [Create a Stream Analytics](#create-a-stream-analytics)
  * [Azure Data Factory](#azure-data-factory)
    + [Create a Data Factory](#create-a-data-factory)
    + [Data Factory components](#data-factory-components)
      - [Activity](#activity)
      - [Pipeline](#pipeline)
      - [Create a Linked Service](#create-a-linked-service)
      - [Creata a Dataset](#creata-a-dataset)
      - [Integration runtime](#integration-runtime)
      - [Triggers](#triggers)
    + [Demo of copy pipeline](#demo-of-copy-pipeline)
  * [Databricks](#databricks)
    + [Mount datalake in DBFS](#mount-datalake-in-dbfs)
  * [Monotoring and optimizing data solutions](#monotoring-and-optimizing-data-solutions)
    + [Azure Monitor](#azure-monitor)
      - [How to use](#how-to-use)
    + [Storage Account Monitoring](#storage-account-monitoring)
    + [Synapse Monitoring](#synapse-monitoring)
    + [CosmosDB Monitoring](#cosmosdb-monitoring)
    + [Data Factory monitoring](#data-factory-monitoring)
    + [Databricks monitoring](#databricks-monitoring)
    + [Stream Analytics monitoring](#stream-analytics-monitoring)
  * [Optimize Data Solution](#optimize-data-solution)
    + [Data partitioning](#data-partitioning)
    + [Optimize streaming analytics.](#optimize-streaming-analytics)
    + [Azure Synapse Analytics](#azure-synapse-analytics-1)
    + [Managing data Lifecycle](#managing-data-lifecycle)


# DP-200

Before the exam were product-based (ex. SQL-Server). Now there are role-based certification (Data Engineer).
You should be able to get requirements, design (DP201) and *implement* (DP200) a solution using services in azure.

Implement what:
- Ingest streaming, batch data and transform them.
- Securitity in mind
- Implement data retention policy
- Identify performance bottleneck

Which Data Solution:
- CosmosDB, Azure SQL Database, Azure Synapse
- Data Lake
- Data Factory
- Azure Stream Analytics
- Azure Databricks

Skill measured:
- Implement daa storage (40-45)
- Manage and develop data processing (25-30)
- Monitor and optimize data solution (30-35)

## Implement Data Storage Solutions

**Storage Accounts** includes 4 distinct services:
- Blob
- Tables
- Queues
- File Share

### Storage Account creation

1. Search for Storage Accounts.
2. Click on Add (+)
3. Select Subscription, Resource Group
4. Insert a name (very restrictive, no capital letters or dashes, it should be globally unique), location (place on the region closest to your client)
5. Performance (Standard, Premium). The SLA is different, performance are faster. Standard uses HDD. Premium uses SSD.
6. Account kind (V2, V1, BlobStorage):
    - V2 default.
        - It supports all the services
        - have different tiers (hot, cool, archive)
        - It supports Block Blob, Page Blob, Append Blob
        - Support for datalake
    - V1 (legacy general purpose, no tiers, no datalake)
    - Blob Storage (legacy), just blob, support block blob and append blob
    - [PREMIUM ONLY] Block Blob Storage: optimized for high rates
    - [PREMIUM ONLY] File Storage: high performance, low latency, iops boosting.
7. Choose Replication:
    - LRS: 3 copies in different racks in the same zone in G1
    - ZRS: 3 copies in 2 or 3 availability zones in G1
    - GRS: (3 copies in 1R) and (3 copies in 2R). You can read from G2 just in case of fail-over.
    - GZRS: (3 copies in different availability zones in G1) and (3 copies in G2). You can read from G2 just in case of fail-over.
    - RA-GRS: like GRS but you can have access to G2 copies even without fail-over.
    - RA-GZRS: like GZRS but you can have access to G2 copies even without fail-over.
8. Choose Access Tier: cool or hot (by default).
9. Click on Networking
10. Connectivity method:
    - Public endpoint (generally reachable in the internet)
    - Specific Public endpoint (you need to choose a Virtual Network)
    - Private network (only from private network, from IPs in the VNET)
11. Data protection:
    - Soft deleted
    - File share soft delete
12. Advances tab:
    - Make sure TLS is mandatory, choose the right version (1.1 and 1.2) go to use the latest if you can.
    - Allow Blob public access: Make sure it is disable.
    - Large file shared: disable.
    - Hierarchical namespace: Enable (DataLake), Disable. Available just on the creation period. Main purpose of the data-lake to store eterogenous data. Optimized for big-data queries.
12. Add tags.
13. Create.

### Storage account tabs

Once created:

- Tab Activity logs: you have the activity log, whenever someones change configs etc.

- Tab: Access key: 2 **access keys** (you can rotate), and you can get the connection string. Access keys gives full access to the storage accounts. Not reccomanded to use it in production.

- Tab Shared Access Signature: If you are creating an application that only reads from storage account, it is not a good practice to share this key. To overcome with this problem you have **SAS (Shared Access Signature.**). You can create keys with limited access in terms of services (Blob), resources (Container, Object), permission (read, list, write etc.), start, and expire date. And you can also limit the IP address ranges that are supposed to use the key. HTTPS only. You are going to use one of the storage account keys to sign the SAS token. If you rotate the key, the SAS token signed with that key will be invalid. After the creation you will not able to see again the SAS token in the portal, the only way that you have to invalidate is invalidating the access-key.

- Tab: Access Control IAM (RBAC and accesses):
    - Go to Activity Directory first, you can create user, group or service principle (identify for app)
    - Go to Role Assignments tags and select the Role
     - Choose the Role (eg. Owner, Contributor etc) and some very specific to Storage Account.
    - Select the user you would like to access the Role to.

- Tab: Access Control List (ACL):
    Another level of permissions at container, folder, files level.
    Go to Azure Storage Account Explorer > Right Click on the resource > Manage Access.
    Default: all the files moved in the future into the resouce will inherit the permission settings.
    You should always try to give permission to groups, or service principles, rather than for each user.
    What about `execute`? To list the content of the folder you need to have the execute permission, or delete recursively. 

- Tab: Networking:
    - Here you have everything about acceses via network, virtual networks and firewall.
    - You can disable access from the public internet and configure accesses of some private network.
    - With firewall you can allow some IPs only.
    - Azure Defender for Storage, monitor the read/write access and pass the traffic to anomaly detectors.

- Tab Encryption:
    - Microsoft managed keys
    - Customer managed keys (you put the key in the key-vault)

- Tab Disaster Recovery:
    - If you have GRS or RA-GRS you can ask to initiate a manual fail-over request to move the primary region to
    your actual secondary region.

We have said that Storage Account has different services: blob, file, table, queue.
You find a set of settings for each of these services.

Under Blob list, you find:
- Containers, where you can create new containers
- There are 3 types of blob:
    - Append blob are optimized for append operations, ideal for logging
    - Page blob are optimized for Virtual hard drive (disks for VM)
    - Block blob are optimized for store text or binary (each blob is divided into small portion, called blocks)

Under File services:
- Folder that you can see in the network (SMB). Set up and copy-paste the script to create the network folder in your PC. Legacy solution, avoid if you can.

Under Table services:
- Table it is a key-value services
- Each entity has partition key (property you need to split your data) and row key (value that is unique within a given partition)
- Then you can add also other columns as well.

If you want better performance -> CosmosDB.
They share the same CosmosDB Table API.

Under Queues service:
- If you want a message queue.

How can I manage and see the objects inside the storage account?
- You can use Azure Storage Explorer (you can see queues, tables, blob, file-share)
- Programmatically (REST API), there are SDK
- Inside the portal

It uses strong consistency that guarantess that when the Storage services commit a data insert or update operation all further access to that data will see the latest update.

## CosmosDB

It is a globally distributed NoSQL database.
Fully managed database as a service, SLA with 99,999.
It stores data as JSON documents. No schema or index managaments.

CosmosDB is a multi-model databases. There are 5 models we can create:
- SQL API (Core API): data are stored as JSON documents.
- Table API: Like Table Storage but more performant.
- Graph (gremlin)
- MongoDB API
- Cassandra API

### Create CosmosDB instance

To create:

1. Click on CosmosDB
2. Click on Add (+)
3. Subscription/Resource group
4. Add name, with restriction and it should be unique
5. Choose the API, you can't change the API later.
6. Choose the closest region to your customers
7. Capacity mode: provisioned throughput.
8. You can apply the free tier.
9. Select geo-redundancy to pair region, and multi-region write options.
10. Go to networking, you can restrict the access to specify particular virtual network or private endpoint
11. Go to Backup (Backup Interval and Backup retention), backup service is not for free.
12. For CosmosDB encryption is enabled and you can't disable it. You can just choose the key that you want (azure managed or not).

ComosDB can't be stopped.

### Access and Security

Go to inside the Azure CosmosDB instance.

- You can access using Keys (2 Keys), no SAS tokens here. You also have 2 read-only keys. You can reset the keys if you want. Other than the keys, you have *Cosmos DB resource tokens* provide a safe alternative that enables clients to read, write, and delete resources in your Cosmos DB account according to the permissions you've granted, and without need for either a primary or read only key.
- IAM (RBAC), you can assign and give access to user/app/groups some role defined in Azure Activity Directory.
- Firewall and Virtual Network, you can select the virtual network able to access or IPs
- CORS: you can whitelist domain names that can access CosmosDB
- Private End-point: all the traffic pass by your private connection to the datacenter
- Advanced Security: audit the access and usage and raise alerts on not commong things.

### Replication and failover

By default your data are replicated different times in the same availability zone (LRS).
You can enable the replicatation in different availability zone from the portal (ZRS) when you create the CosmosDB.

I can copy my DB in different regions (for performance and for disaster recovery).
If you do a replica, you are going to pay twice the price you will pay with 1 replica.
Moreover, you are going to pay for extra traffic for sync your data.
By default the second region will have the read-only permission, but you can enable to have read/writes permissions (**multi-region writes**). 

You just have to go in the portal and do a click to replicate data in different regions.

#### Multi write conflict managment

If we enable multi-writes, we can have conflicts in updating the same record in different locations.
We should define a logic to provide a solution. To define a solution:

- Go to `Database > Container> Scale & Settings > Conflict Resolution`.
- Last Write Wins (you can set the property you want to evaluate) or Merge Procedure (with custom logic).
- If you select merge procedure but not providing any logic, the record goes in some conflict cluster and then you
can decide manually which one to keep.

#### Failover

In case of disaster. If you have already write enabled region. There is no action that you are required to do.
But if you have just one region write enabled and the other replicas are read enabled.

Case 1 - If one of the read region goes down: all the user will be redirected to the nearest region.
Case 2 - Write region fail:
- Manual Failover: and choose the datacenter you want to enable as write enabled. It takes time for doing the manual failover.
- Automatical failover: you can choose which region you want to promote to write enabled when a data center goes down. Your user will not perceive.

#### Backups

If you accidentally delete your data or corrupt your data?
You use the backup service, no additional cost.

2 backup possible.
Interval 4h.
Retention 8h.

You can change *interval* and *retention* in the portal in the tab `Backup and Restore`.
Where do they store it? They store it it in `blob storage service`, in your region and also in your paired region.
To restore from a backup you have to open a *Support ticket* for it.

### Consistency

Data that your user is going to see when the data are replicated in different regions.
What the user see? It takes some time for the update to propagate. Specifying the level
you specify how tolerante you are.

5 level of consistency you can choose.

- Strong (you wait, but you will read always the latest data)
- Bounded Staleness (dirty read possible, bounded by time and updates)
- Session (within the same session you have strong consistency, in different session you have consistent prefix)
- Consistent prefix (ordered sequence is guaranteed)
- Eventual (you read, but it is no guarantee that you will read the latest data, neither the ordered sequence is guaranted).

You decide based on the application. Seeing number of likes is one use case where it is ok for the user to see
a wrong number of likes for a while. However, for a bank application the user have to see always the latest data.

Why don't I go with the strong consistency? User everywhere see the latest data. You can, but it adds latency.

We can set a default consistency for the entire account.
But we can override the consistency level at request level (but only if it is a level weaker than the default level). 

### Database/Container/RU

One account can have one or more databases.
Inside a database you can have one or more containers.
You can have different names for databases and containers based on the API you have decided.
For example, container in MongoDB are called Collection.

You can can't query cross containers.

Latency, user await to have the result back.
Throughput, how much workload the database can handle.

How we define the throughtput? Request Unit (RU/s).
Request Unit is a combination of Memory, CPU, Disk read/write.
It is very transparent, you can see for each query that you run
how much Request Unit you have consumed. If you run the same
query you are going to consume always the same RU/S.

From the data-explorer, you can create a database and container. We have learnt that a container contains JSON file at the end.
When you create a container, you have to specify the number of Throuput units you want to assign, or if you want to put autoscale. You are going to pay for throught-units. 

You can set the throuput in two levels: database level or container level (dedicated). You can decide it just for the time
of the creation. You can mix database level (shared thoughtput among the containers that does not have a dedicated througput) and container level.

### Partitions

We put data in a single container.

Behind the scene, the container writes data into a series of physical machines in which we store data and we use them
to perform the computation. Those machines can scale horizontally.

We insert data into the single container, but the data is distributed across all these machines (physical partition)
 
We can specify for each container a `partition_key` (eg. *partition_key = city*). You define the partition at the time of the creation.

And end up with `logical partitions` where the data are divided based on the values of the partition keys we have divided.
eg. partition for `london`, partition for `rome` etc.

Internally, one or more logical partitions are mapped to a single physical partition. You will not see data of one logical
partition divided into two physical partitions. CosmosDB can move entirely a logical partition from one physical location
to another physical location.

We are in control of the `partition key` but we are not in control of the `physical partition`.

#### Parititions and throughput

All the partitions share the same throughput of the container.
If the container has 10.000 RU assigned and I have 5 partitions. Each of the 5 partitions will have 2.500 RU assigned.
What happened if a logical partition may need more? It will throw an error and you have to buy more RU for the container.

So want absolutely that our data (and queries) are equally distributed among the logical partitions.
A good partition choice could be the `user_id` or `product_id`.

#### CosmosDB Limitations and composite keys

There are some restriction in CosmosDB.

- One document cannot exceed 2MB of data. To overcome this limitation, you can split your documents into many small documents and reference them in a file that operates like an `index`.
- Each single logical partition can't exceed 20GB.

Instead of choosing `customer-name` or `date`.
We can use a paritition key a composit keys made up by the concatenation of two of multiples fields `customer_name-date`.


### Manage the data inside

#### Insert/Query data

You can use REST API, SDK or Data Explorer (integrated in the portal).

1) Create Database specifing ID (`Products`), Container ID (`Cloathing`), throughtputs, partition-key `/productId`.
2) Open Products > Cloathing > Items
3) Create or upload a new Item. Once the item is created, there are some metadata appended in the JSON schema (eg. unique document id `_rid`).
4) I can create a second Item, with a different structure.

```sql
SELECT p.id, p.name
FROM Products p
WHERE p.id = "1"

or

SELECT *
FROM Products.shipping.weight -- it will skip the items that do not have such field
```

#### Time to Live

Go to Products > Cloating > Scale & Settings > Time to Leave and specify the number of seconds the items should be deleted automatically.

It works on background. It will use left over RU. If no RU, deletion is delayed.


## Relational Database

### Azure SQL Database

Fully managed enterprise SQL Server database.

A lot of features:
    - Provides predictable performance and price.
    - Elastic pools
    - SLA 99.99
    - Geo Replication
    - Scaling for more performance
    - Secure

Two options to run:
- SQL Server inside a fully managed VM in Azure. Can be good when you want to migrate your database, having full control.
- Use it as Platform as a server: Azure SQL Database. Useful for scaling, pay-as-you-go etc.

We will focus on the Azure SQL database as PaaS.

There are different deployment options:

- Single Database: Single database which its own resources (compute, memory, storage)
- Elastic pool: Resources shared to the pool of databases.
- Managed Instance: Use it when you want to do a migration of existing database, database depoyed in Virtual Network,
                    you have a lot of control but still have some PaaS capabilities.
 
### Create a SQL Database

1. Search for SQL Database
2. Click on New
3. Add Subscription, Resource group
4. Database name
5. Server: it is a logical server, kinda of container for multiple database. You need to add admin username, password, location.
6. SQL Elastic Pool: if you want to have multiple database sharing the same resources
7. Compute + Storage. Clicking on it you can:
    vCore based option.
        General Purpose
        Hyperscale
        Business Critical
    Standard:
        Basic
        Standard
        Premium
    You can choose DTU.
    Disk.

    We will go in detail later.
8. Network, public or private endpoint or use Firewall to specify a range of IPs or virtual networks.
9. Additional Settings, you can decide if you want to start with a blank database or with a sample of data.
10. Add Tags.
11. Create.

Once created you have a:
SQL Database instance and a Server instance.

Clicking on the Server instance. You can manage different aspects: backup, seeing all the database, deleted databases.
If you go in the overview, there is a button to create new database or new pool, or new datawarehouse.

Clicking on the Database we can do to *Query editor*.
Insert username and password of the server.
Once you connect, the tables are in `Database, db_name, Tables`.

### Pricing models

For PaaS you can choose among:
    - Single Database
    - Elastic Pool
    - Managed Instance

2 Purchases models.
- DTU (CPU, Memory, IO), best for pre-configured resources
    - Basic
    - Standard (99,99)
    - Premium (99,99, storage is faster)
- vCore, flexibility to adjust individual resources, reccomanded
    - General Purpose (99,99)
         you can decide how many vCores we need and Data max size
         you can decice to go decide to go `serveless` that will
         scale up the resources 
    - Business Critical (99,99, storage is faster, up to 4TB)
    - Hyperscale (intended for large database up to 200TB)

You can change the vCors and pricing model everytime from the Database Instance and `Configure` tab.

### Elastic Pool

Elastic pools is a set of databases that shares the same of DTUs.
Good if there are databases that are idle and then have some peaks.
It is a cost effetive solution for unpredicatable usage demands.

1. Create 2 single database single instances under the same server (with different price models!) and another 1 in another server
2. For now we select "No elastic pools"
3. Place DB1 and DB2 in an elastic pool
3. Search for Elastic Pools and Click Add
4. Select the server from DB1 and DB2
5. You can chose the number of vCors and Data max size you want to dedicate to the tol
6. Add Tags
7. Create.
8. Once it is created. Go to the instance.
9. To add existing database click on "Configure". Here you can change databases and resources.
10. Clicking on database you can select DB1 and DB2. You can't see DB3 because it is in a different server. You have to migrate DB3
into the first server first.

In putting DB1 and DB2 into the SQL pool. The Pricing tier is now `Elastic General Purpose` for both of them.

### Security

Network security: allow requests based on specific IP firewall rules or requests coming from a subnet inside a virtual network
Access Managment: 
    Authentication: we have SQL authentication (adming with usename and password) and AD authentication
    Authorization: Row level security, control access rows based on the database role of the user.
Threat Protection:
    Azure Monitor Logs: track database activities
    Advanced Threat Protection: analyse SQL server logs to detect unusual behavior or attacks
Information Protection:
    Encryption in transit.
    Transparent Data Encryption, encryption
    Data Masking
Vulnerability Assessment: discover and remediate potential database vulnerability
Data discovery and classification: identify and label sensitive data

### High Availabilty

- Standard Availability Model: for basic, standard, general purpose
    If a primary node goes down, we switch to another node (failover).
    We are going to lose the cache that we have in the primary node so
    there will be some performance degradation.

- Premium Availability Model: for premium , business critical service tier
    You have the primary node and a exact replica of the primary node including cache.
    If you do some fail-over you will not experience any degradation.
    Moreover, you can use the secondary node as read-only node (enable `read scale-out option`)

#### What about an availability zone goes down?

Supported in Premium and Business Critical (enable `zone redundancy`). No extra cost for it.
The same of the HA setup. But the secondary replica is deployed in another region.

#### What about a region goes down?

Primary logical server
Secondary logical server (*in another region* or in the same region of the primary logical server)

Data from primary will be replicated in the secondary logical server.
The secondary copy can have some data lag. You can read from the secondary region, but please make aware
that you can read not exactly the latest data.

Use cases:
- Disaster recovery
- If you want to migrate the database to another region.

For enabling it, go to the SQL database instance in the portal, go to `Geo-Replication`, select the secondaries regions and setup
a server (name, username and password). You can add up to 4

There are two types of fail-over:
- Simple, our application stop the connection from the primary server, it takes some time for the secondary for sync completely with the primary. 
- Forced fail-over: primary put down immediately, maybe some data loss that were not completely replicated.

There is no automatic fail-over. You have to go to `Geo-Replication tab, Right Click on the secondary region, Initiate Failover.` 
After the fail-over, the secondary logical server will be the primary ones, and you have to changes the connection string from the application to this new server.

How to solve this 2 problems?

Go creating a `Fail over group` just specifing the primary server, secondary server.
-  You can set an automatic failover only if you define a (you have to decide the grace period you want to wait after the outage).
- You can also have a read/write and read-only endpoint that will never change in case of failover.

With failover group you can fail-over multiple database at one time. There are some limit however in using fail-over groups.
For example you can't have multiple replicas but just one.

#### Backups

You have automatic backups that will let you come back within 35 days.
By default, SQL Database and SQL Managed Instance store data in geo-redundant storage blobs that are replicated to a paired region.


## Azure Synapse Analytics

Modern DWH is composed of this steps:
1. First thing, bring the data from different sources (structure, unstructure etc.).
2. We need to ingest the data into storage (from source to storage). For example, data lake architecture.
3. Data can be explored.
4. Prepare, clean the data.
5. When data are prepared, we create a structure model of the data, creating a single source of truth, where can query.
6. Then there is the visualisation.

Computation and storage are separated.
For the computation: DWU - Data warehouse unit (CPU, Memory, IO). You can pause the DWH and do not consume DWU.
For the storage: you pay for the cost.

Once the data are in the datalake, DWH have polybase capabilities to get data from the datalake and bring to DWH.
Or you can use Databricks to explore, transform the data (spark environment). You typically publish the data into DWH, where
business users can query. It is performant DWH solution.

Synapse Analytics is an service that combine all this services to have an e2e analytics service.
You can still access to individual services.
DWH in Azure is now called `Dedicated SQL Pool`.


### Create new Azure Synapse Analytics - SQL Pool

1. Subscription, Resource Group
2. SQL pool name
3. Create a new server, logical container for databases, You need to specify unique name, username, password, location.
4. Performance Level:
    Gen2
    Just select the DWU you want (min.100)
5. In additional settings:
    - You can create a clean database or with data sample.
    - Add tags

Going to resource you are in Synapse SQL pool.

Once you have created the pool you can Launch the **Azure Synapse Studio**, that is the tool you use to interact with the services.

- Home: Shortcut for functions
- Data: Clickin on Data you have two tab, workspace (for tables) and linked (for accessing data lake stuff).
- Develop: where you manage SQL scripts, synapse notebooks (spark), data flows (code free GUI with spark on background), powerbi reports
- Integrate: like Azure Data Factory, scheduling pipeline (code free), basically drag and drop components
- Monitor: view pipeline and triggers runs, view the status. Grouped into "Integration" and "Activities".
- Manage: perform same action of the portal like managing SQL and Spark pools,manage Linked Services, create pipelines trigger.

To connect to SQL Managment Studio
- Go to Settings > Connections Strings. TCP port 1433.
- Copy Server Name
- Use SQL Server Authentication and add server name and server password

You will fail to login because you have to change the IP address rule in the firewall settings.
Specify a Client IP address you want.

### Backups

- geo-backup once a day. Time for restore is 24h. By default and you can't change it.
- you also have snapshots every 8h, in the past sevent days.

### What is MPP

MPP = Massive Parallel Process Architecture.
DWH is MPP system. To do query in parallel.

On the top we have the control node, that is the single point of entry to the DWH.
The single node has the MPP Engine, that contains the engine that is used to split the query
in several parallel queries to different `Compute Node`. Compute node queries then the data in
the Azure Storage.

Number of compute node is determined by the DWU. `Compute Node` do not have data into their storage. But the storage is in the Azure Storage instead. A `Compute Node` has something called `DMS` (Data Movement Service), that is used to share the partial result among the other workers nodes. When the parallel computation is done, the result is then shared to Control node.

#### Sharding Patterns

Distribution is the basic unit of storage and processing for parallel queries.
Rows are stored in 60 distributions which are run in parallel.
Each compute node manage one or more of the 60 distributions.

DWH has 3 Sharding Patterns.
Meaning, how to distribute the data among the distribution.

- Hash: Highest query performance for JOINs. Hash function to assign each row to a distribution. Hash distribution used for large tables. It is important that the hash key distribute evenly, used for grouping, data for joins are in the same distributions. Distribution key can not be changed. (usually for facts table)
- Round robin: distribute data evenly among all the nodes. Joins are a bit slow, cause data needed end up in different distributions. Great for temporary table, or if you do not have any good candidate joining key. (usually for dimensions)
- Replicated: table is replicated in the compute nodes. No need to trasfer data in nodes. Used with small tables.

#### Table types

- Clustered Columnstore: Default table store. Meant for large tables.
- Heap: no index on the data, data are not ordered. It is fast for load.
- Clustered B-Tree Index: table organized on a sorted column. Will provide no compression.

#### Table partitioning

Enable you to divide the data into smaller groups.
Data is traditionally partition based on the data the data have arrived or based for query logic.
However, you do not want too many partitions, a highly granular partitioning scheme can hurt the performance.

### Loading Method

- Single Client loading methods: for example, Azure Data Factory, can be bottlenecked by the control-node.
- Parallel readers loading methods: Polybase, bypass the control node loading the data directly into the compute nodes and the distributions. Reads from Storage and Load the content in parallel.

### Set up Polybase

1. Create a master key and scoped credential with the storage key
3. Create an external data source
4. Create external file format
5. Create an external table
6. Bring the data.

We start from having a storage account containing the flat file we want to load as Polybase.
Click on Access Keys and copy the Key and the Storage account name to a notepad.

#### Creates Master Key and securely store storage Credentials

The next step is to create a database scoped credential to secure the credentials to the ADLS account. 
The database master key is used to encrypt the private keys of certificates and keys that are present in the database.
You can run the following query from the Query Editor directly in the portal.

```sql
-- Create a database master key if one does not already exist, using your own password. This key will be used to encrypt the credential secret in next step.
CREATE MASTER KEY ENCRYPTION BY PASSWORD = 'Pa$$word123' ;
CREATE DATABASE SCOPED CREDENTIAL ADLS_Credential
WITH
-- IDENTITY = '<storage_account_name>' ,
-- SECRET = '<storage_account_key>'
    IDENTITY = 'dlspolybasestorage' ,
    SECRET = 'zN9S8mggblYX/sEiz5DVRmcZWjPw65A393bzD9T5rQjo+LnI5GAGrjdLvt4iqK5YEWMSMqV82IsVm3Bww6uw=='
;
```

#### Create an external data source

Use the database-scoped credential to create an external data source named AzureStorage. The location URL point to the container named csvstore in the ADLS Gen2 account. The type Hadoop is used for both Hadoop-based and Azure Blob storage-based external sources. Modify the location to refer to your storage account and the container.

```sql
-- Note this example uses a ADLS Gen2 secured endpoint (abfss)
CREATE EXTERNAL DATA SOURCE AzureStorage
WITH
  ( LOCATION = 'abfss://csvstore@dlspolybasestorage.dfs.core.windows.net' ,
    CREDENTIAL = ADLS_Credential ,
    TYPE = HADOOP
  );
```

#### Create external file format

The external file format object contains how the data in a file is structured and defines how rows are defined and what column separators are used.

```sql
-- Create an external file format for DELIMITED (CSV/TSV) files. 
CREATE EXTERNAL FILE FORMAT csvFile
WITH (
    FORMAT_TYPE = DELIMITEDTEXT,
    FORMAT_OPTIONS (
      FIELD_TERMINATOR = ',',
      STRING_DELIMITER = '"',
      FIRST_ROW = 2,
      USE_TYPE_DEFAULT = FALSE,
      ENCODING = 'UTF8' )
);
```

#### Create external table

The external table object uses the external data source and external file format objects to define the external table structure within Azure Synapse Analytics. You can then use the external table as a basis for loading data into your data warehouse.
Create an external table named dbo.FIPSLOOKUP_EXT with the column definition corresponding to your CSV file. Use a WITH clause to call the external data source definition (AzureStorage) and the external file format (csvFile) we created in the previous steps. The location denotes that the file to load is in the root folder of the data source.

```sql
-- Create a temp table to hold the imported data
CREATE EXTERNAL TABLE dbo.FIPSLOOKUP_EXT (
   UID INT NOT NULL,
   iso2 VARCHAR(2) NULL,
   iso3 VARCHAR(3) NULL,
   code3 INT NULL,
   FIPS INT NULL,
   Admin2 VARCHAR(255) NULL,
   provincestate VARCHAR(255) NULL,
   countryregion VARCHAR(255) NULL,
   latitude DECIMAL(12,9) NULL,
   longitude DECIMAL(12,9) NULL,
   combined_key VARCHAR(255) NULL,
   population INT NULL
)
WITH (
    LOCATION='../',
    DATA_SOURCE=AzureStorage,
    FILE_FORMAT=csvFile
);
```

We can already run the query to see if the data looks correct.
Right click on `Tables > External Tables > dbo.FIPSLOOKUP_EXT` run query and do a select.

#### Load destination table

We have set up our PolyBase correctly; however, the data is not yet physically stored in our data warehouse. The data still exists only in the ADLS account. We need to load this data into a physical table to persist in our data warehouse physically.

Create a physical table in the Azure Synapse Analytics. The table will have a `clustered column store index` defined on all the columns with a `round-robin` table geometry because round-robin is the best table geometry to use for loading the data.

```sql
-- Load the data from Azure Data Lake Store Gen2 to Azure Synapse Analytics data warehouse
CREATE TABLE dbo.FIPSLOOKUP
WITH (   
    CLUSTERED COLUMNSTORE INDEX,
    DISTRIBUTION = ROUND_ROBIN
)
AS
SELECT * FROM dbo.FIPSLOOKUP_EXT;
```


## Securing Data Platform

Security is a shared responsibility with Azure.

There is the concept of defense in depth where there are several layers protecting your data.
0. Data
1. Application, ensure application are secure and free of vulnerabilities, store sensitive secrets in secure storage etc.
2. Compute, secure access to virtual machines and keep system patched
3. Networking, deny by default, limit communication, implement secure connectivity
4. Permiter, DDoS protection and firewall
5. Identity and Access, control access, using single-sign-on, multi-factor, audit events
6. Physical Security

In the Azure Security Center you can monitor the security and have reccomandations.
Two pricing: Free and Standard full suite including continuos monitoring, just-in-time access controls for ports)

### Key security components

About Network and Perimiters.

- Firewall, managed cloud-based network security service that protects your network. It is a stateful firewall. Protect against suspecious requests and have in-bound/out-bound rules.
- Application Gateway is a load balancer with a smart firewall.
- DDoS (Basic and Standard). Protect against DDoS attack. Standard gives you cost protection, service availability.

Data Stores such as Azure SQL Database and Data Warehouse has a built-in firewall that is used to allow and deny network access to both the database server itself, as well as individual databases. 

Firewall rules are configured at the server and/or database level, and will specifically state which network resources are allowed to establish a connection to the database. Depending on the level, the rules you can apply will be as follows:

Server-level firewall rules:
- Allow access to Azure services
- IP address rules
- Virtual network rules

Database-level firewall rules
- IP address rules

### Azure Defender for Storage

It is a premium service for the Storage Accounts.
Go on Security > Advanced Security > Azure Defender for Storage.
It discover anomalies in accesses and send alerts.

### Managing Authentication and Authorization

Even though we may be able to connect to the database over the network, that doesn't mean we can actually gain access to the data itself. SQL Database and Data Warehouse supports two types of authentication: 

- SQL authentication: use username and password, granted by the admin of the database within the database itself.
- Azure Active Directory authentication: uses identity managed in AAD. You should create and Azure AD admin that is alllowed to administer Azure AD users and groups.

Once you are Auth. You can grant permissions to users and group in the database itself. Like:

```sql
-- create user and password
CREATE USER ApplicationUser WITH PASSWORD = 'YourStrongPassword1';
GO

ALTER ROLE db_datareader ADD MEMBER ApplicationUser;
ALTER ROLE db_datawriter ADD MEMBER ApplicationUser;

--- We can narrow the scope of access further.
DENY SELECT ON SalesLT.Address TO ApplicationUser;
```

You can also use SQL Server Studio for running queries of course.

### Monitoring Database accesses

By enabling auditing, operations that occur on the database are stored for later inspection or to have automated tools analyze them. Auditing is also used for compliance management or understanding how your database is used. Auditing is also required if you wish to use Azure threat detection on your Azure SQL database.

Audit logs are written to Append Blobs in an Azure Blob storage account that you designate. Audit policies can be applied at the server-level or database-level. Once enabled, you can use the Azure portal to view the logs, or send them to Log Analytics or Event Hub for further processing and analysis.

For activating auding go to: Security > Auditing. It is disabled by default. Then click on `Storage details` for specifing the Storage Account into which save the logs.

If you want you can enable also `Advanced Threat Protection for Azure SQL Database` that will analyse the logs to look automatically into potential problems. To activate it, go in `Security > Advanced Threat Protection` then you can specify the `Detection Type (SQL Injection or anomalous access`). You should go in the vulnerability Assessment box, you should see a message that says “Click to configure a storage account for storing scan results” and then choose a Storage account. You can also configure periodic scans.

For CosmosDB you have `Azure Diagnositics logs for Azure Cosmos DB`.

### Data Masking

You can store information like credit card.
You do not want normal database users to see this data.
You can mask those fields when those users query the table.
Admin users have the full priviliges and can see all the data even if masked.
Masking is good but users can do some brute force quering. You can't use the data masking as only way to mask the data.

Different functions:
- Default function, in any string or in any number. It completly mask the entire field.
- Random number function, takes the min and max values and gen a random number inside. It works only with numeric values.
- Custom String function, you can apply only for string. You can decide which character to mask (all chars but first char). You can specify the details,
- Email functions
- Credit cards function, it shows just the 4 digits.

How to setup a masks?

1. Go to portal > Security > Dynamic Data Masking
2. Select the table to mask, select the column and the function
3. Click add

You have to do column by column.

You can do the masking directly using queries:

```sql
CREATE TABLE Membership
FirstName varchar(100) masked with (function 'default()')
-- or
ALTER TABLE Membership  
ALTER COLUMN LastName ADD MASKED WITH (FUNCTION = 'partial(2,"XXX",0)');  
```

### Encrypt data at rest

Data at rest means that they are stored. We are not talking about `in transit data` where data are in motion in the network.
All data services have encryption at rest by default, sometimes it is not possible even to disable it. You can configure
it using your own symmetric key or one manage. You can choose from the keyvault.

Storage account, encrypted. Can't be disabled.
Storage account gen2, encrypted. Can't be disabled.
SQL Database, encrypted. `Can be disabled`. If you go to the server you can decide the key.
Synapse SQL Pool, encrypted. `Can be disabled`. If you go to the server you can decide the key.
CosmosDB, data always encrypted. Can't be disabled.

In SQL Database you can encrypt again the fields from the SQl Managment Studio. You can set `deterministic encryption`
or `randomized encryption`.


## Stream Analytics

Fully managed, real time analytics service to process fast moving streams of data.

Stream source:
- Event Hub
- Azure Blob Storage
- IoT Hub

Input source for reference data - static data used for transfomation purposed:
- SQL DB, Blob Storage

Ouput:
- Event Hubs
- PowerBI
- Synapse
- Storage, DB

Every event that comes as a timestamp. Based on the timestamp we can create buckets of input data.
For example, take batch every 5 seconds. These buckets are created based on a Window function. There
are more Window functions. We can perform operation on a window level (doing, count, avg and so on).

- **Thumbling Window**, take a bucket every X time unit (eg. every 10 seconds)

```sql
SELECT TimeZone, COUNT(*) AS Count
FROM TwitterStream TIMESTAMP BY CreatedAt
GROUP BY TimeZone, TumblingWindow(second, 10) -- bucket created every 10 seconds
```

- **Hopping Window**: a bucket of length X, taken every Y seconds. There is overlap.

```sql
SELECT TimeZone, COUNT(*) AS Count
FROM TwitterStream TIMESTAMP BY CreatedAt
GROUP BY TimeZone, HoppingWindow(second, 10, 5) -- bucket created every 5 seconds, should hold for 10 seconds (lenght)
```

- **Sliding Window**: fix length (eg. 10 seconds), when a new event happen. It looks to 10 seconds behind window. There could be overlap.

```sql
SELECT TimeZone, COUNT(*) AS Count
FROM TwitterStream TIMESTAMP BY CreatedAt
GROUP BY TimeZone, SlidingWindow(second, 10) -- slide back for 10 seconds from the current event
```

- **Session Window**: length of the window is not fixed, there is no overlap.

```sql
SELECT TimeZone, COUNT(*) AS Count
FROM TwitterStream TIMESTAMP BY CreatedAt
GROUP BY TimeZone, SlidingWindow(minute, 5, 10) -- start when it receive an event, if it does not receive events for 5 minutes it breaks. After 10 minutes it breaks anyway.
```

### Create a Stream Analytics
 
1. Open Stream Analytics Jobs
2. Add
3. Name the job, subscription, resource group, location
4. Hosting environment: Cloud, Edge (deploy stream analytics to container you can deploy in IoT device)
5. Specify streaming units (more they are, more it is performant)

There is a play button,
You can see the query.
And you can select the input/output.

Job topoligy is important, and it is the section we use to set up the job.

In the job topology, you need to specify:
- Input Tab, you can add the streaming input (Add Stream Input), in our case, the EH. You have two input label, one is Streaming Input and then you have Add referece input look-up tables, metadata later joins (SQL table, blob).
- Output Tab: for example blob storage. You can set the minimum rows, hours, minutes to write down the input.

- Streaming query tab: where we define our USQL query.

Our data looks like this.
    - ID: GUID
    - TEMPERATURE
    - SENSOR_ID

For defining the query you have to specify a USQL query.

```sql
SELECT
    System.Timestamp() AS EventTime, AVG(eventhubInput.Temperature)
INTO
    BlobOutput
FROM
    eventHubInput
WHERE eventHubInput.temperature IS NOT NULL
GROUP BY TumblingWindow(second, 5) <------- average over the data that came in the last 5 seconds.  
HAVING AVG(eventHubInput.temperature) > 100
```

You can start the job now or you can schedule it.


## Azure Data Factory

Fully managed cloud based integration services. It has more than 200 services, but 3 main tasks

- Migration data
- Transform Data, it has some tool for simple transformation, called Data Flow.
- Coordinate data workflows, the actual work is done by other services (eg. databricks)

Data Factory is composed of 4 components:
- Linked Service: linked service as source for data or that can be triggered for compute service
- Datasets: Azure Data Factory is made aware of the datasets that it should use
- Activity: contain trasformation logic or the analysis command of the Azure Data Factory work. 
- Pipeline: Multiple activity takes the name of pipelines, that can be scheduled or answer to a trigger. You can also specify parameters as key-value pairs.

### Create a Data Factory

1. Create Azure Data Factory:
    - Using Portal: you must have `Data Factory Contributor` role.
    - Using SDK: you must have `contributor` or above.

2. You have to specify subscription, resource group, region, name and version (V2 is the latest). You can enable git ingreation.

3. Once it is created you go to `Author and Monitor` to start using data factory.

In the Menu you have:
- Data Factory: home page
- Author: where design the pipeline
- Monitor: monitor the pipeline
- Manage: to manage the linked services, integration runtime, and so on

### Data Factory components

Integration Runtime: provides the infrastructure to run any of the activity.
Dataset: You have the definition of the data that Data Factory has to manage.
Linked Service: how to connect with the source, destination and so on (connections strings and credentials)
Activity: consume and produce Dataset connecting using Linked Services (Storage or Computation.)
Pipeline: is logical group of activity.

#### Activity

There are three categories of activities:

- **data movement activities**: simply move data from one store and another.
- **data trasformation activities**: You can perform some within Data Factory with a tool called Mapping Data Flow, or you can use service for transformation like Databrcisk, SQL database, Synapse etc.
- **control activities**: other activity that you can put in your control flow, like ForEachActivity, WebActivity, Until Activity etc.

An `Execution` activity looks like this


```json
{
    "name": "Execution Activity Name",
    "description": "description",
    "type": "<ActivityType>",
    "typeProperties":
    {
        // properties that depend on the activity type, , like script to execute
    },
    "linkedServiceName": "Databricks", // name of the linked service
    "policy":
    {
        // like timeout, retry behaviour
    },
    "dependsOn":
    {
        // used to define activity dependencies
    }
}
```

While a `Control` activity looks like:


```json
    {
        "name": "Control Activity Name",
        "description": "description",
        "type": "<ActivityType>",
        // there is no linked service and policies
        "typeProperties":
        {
        },
        "dependsOn":
        {
        }
    }
```


#### Pipeline

Go in Author menu.
Inside the Author we have a sub-menu:

To define pipelines, click on pipelines you have a menu where you can drag-and-drop activites. Selecting the activity on the bottom you have all the options and configuration for the selected activity. On each activity, you can add a trigger based on the result of the activity (if success, if fail...) and link more activity forming the pipeline. Behind the scene the interface create a json-code.

A `Pipeline` looks like that:


```json
{
"name": "PipelineName",
"properties":
    {
        "description": "pipeline description",
        "activities":
        [
            // list of activities here
        ],
        "parameters": {
        }
    }
}
```

#### Create a Linked Service

You can create a Linked Service from the Source menu of many activities directly. Or you can go in the menu `Manage` and create a linked service from there. You have to decide the service you want to connect with (more than 80 connectors). Let's say we decide to create a connection with our own storage account, we need to define a name, and then define the subscription, name of the storage of the connection string etc.

#### Creata a Dataset

1. Let's go to: Author > Datasets
2. Create new Dataset.
3. Select again the storage account, select the type of the text we want (eg. CSV) and then let's add dataset name, and define file path and some other properties (has header, from where to import the schema). You can specify more option after the creation in the bottom menu: for example the separator etc.


**Create a Linked Service and create a Dataset**: there are over 80 connectors. To create a new connector add the Activity `Copy Data` and then you are asked to define a service. Then you will have to specify the connection strings (for example for SQL Database or Azure Storage). Go to `Source > Source Dataset` a click the service you want, let's say `Amazon S3`, then `File format list` select `Delimited Text` and continue (there are also parquet files). In `Set properties` you can give an name to the dataset and then insert `access keys` or `connection string` necessary to connect to the service you have decided. At the end, specify the file-path that you want to use (as input for example).

Example of dataset in JSON

```json
{
  "name": "AzureBlobOutput",
  "properties": {
    "published": false,
    "type": "AzureBlob",
    "linkedServiceName": "AzureStorageLinkedService",
    "typeProperties": {
      "folderPath": "datacontainer/partitioneddata",
      "format": {
        "type": "TextFormat",
        "columnDelimiter": ","
      }
    }
  }
}
```

#### Integration runtime

Fully managed servless compute infrastructure. 

Who actually perform the action/
In the Manage section, you find Integration Runtimes.
You can create more Integration Runtime.

Capabilities from the Integration Runtime
- Data Flow activity, transformation
- Copy data: it copies, serialize, deserialize etc
- Acitivity Dispatch
- SSIS package

We have 3 Integration Runtime
- Azure: if source, dest are in UK. You have several options
     Auto resolve: azure data factory run the integration runtime close to the data location. (DEFAULT)
     UK: do force your data to do not leave the UK
     etc
- Self hosted: if you have data in private network, you have to run integration rumtime in private network too
- Azure SSIS


#### Triggers

Triggers the pipeline.
Click on "Add Trigger".
- Trigger Now
- Create custom Trigger
    Insert name
    Description
    Type
        Schedule, cron expression (until a certain date)
        Tumbling window: Start date, every X minutes or hours. Into advanced, you can put dependencies between other triggers, offset and windows size
        Event: if storage account has new file, a new blob is created or deleted etc


### Demo of copy pipeline

1. Go to Author Page.
2. New Pipeline, insert name, description.
3. Drag and drop the `Copy activity pipeline`
4. In the options, go to **Source** and select a Dataset, if you don't have one you can create one.
5. Creating a new Dataset
    - Select Blob Storage
    - File Type (CSV)
    - Select the linked service for the blob storage
    - Choose the file
    - Import schema from the same file directly
6. Go to `Sink` and select a Dataset otherwise you can create a new one.
    - Select SQL Server
    - Select the defined linked service
    - Select Destination Table
7. Go to mapping:
    It shows you the mapping from the source to the destination columns. You can change some type etc if you need.
8. Go to settings
9. User Properties:
    Values used to monitor it during the monitoring.
10. Click Validate to look if everything is good.
11. Run DEBUB if you want to make a run.
12. Publish the pipeline.
13. Add trigger if you want.
14. Open the monitor section to see the progress.



## Databricks

Fully managed cloud data and machine learning platform.

### Mount datalake in DBFS

Seemless access to data from the storage account without requiring credentials.
To authenticate we use `Service Principle` (application identity in Azure Activity Directory).

In Azure Active Directory:

1. Go to Azure Activity Directory
2. Go to `App Registrations`
3. New Application
4. Add Name and Click on Register
5. You have `Name`, `Application ID` (identifies the aplication) and `Directory ID` (identifies the azure active directory)
   Go to `Certificate and Secrets > + New Client Secret`. Store the secret to the Azure Key Vault.

In Azure Storage Explorer:
6. Open the Azure Storage Explorer and Right Click on the container > Permissions.
7. Add and search for the service principle by name, and give all the access permission to the cluster. Now our databricks application in activity directory has access to the Data Lake.

In Databricks:
8. Run all the commands to mount

```python
configs = {"fs.azure.account.auth.type": "OAuth",
           "fs.azure.account.oauth.provider.type": "org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider",
           "fs.azure.account.oauth2.client.id": "<application-id>",
           "fs.azure.account.oauth2.client.secret": dbutils.secrets.get(scope="<scope-name>",key="<service-credential-key-name>"),
           "fs.azure.account.oauth2.client.endpoint": "https://login.microsoftonline.com/<directory-id>/oauth2/token"}

# Optionally, you can add <directory-name> to the source URI of your mount point.
dbutils.fs.mount(
  source = "abfss://<file-system-name>@<storage-account-name>.dfs.core.windows.net/",
  mount_point = "/mnt/<mount-name>",
  extra_configs = configs)
```

It is reccomanded to add the configuration in the cluster settings.

## Monotoring and optimizing data solutions

### Azure Monitor

Open `Azure Monitor` service. It is a service where you can monitor all your azure services and collect and analyise logs coming from those services.

From where does it collect metrics and logs? Application, Operating System, Resource, Subscription, Tenant and Custom Sources.

Azure Monitor:
- Insights: analyse those logs and provide a quick insight
- Visualise: you can create a dashboard visualizing data
- Analyse: you can run custom SQL query providing custom logic to analyse the logs
- Respond: you can set alerts based on condition on certain logs.
- Integrate: enable you to integrate with different services (for example, sending events to event-hub)

#### How to use

- Activity Logs: you can see the operation performed on all the resource (change access key, delete, create some account etc)
- Alerts: you can create a new alert. You select what you want to monitor, and then the condition, then you set an action group (sms, email, autoscale etc)
- Metrics: select the resource to monitor, you can select different charts, for example on fails.
- Logs: you can query logs
- Service health: see if there are issues on azure side
- Workbooks: where you can see some overview, dashboards already created and that can be created.
- Insights: behing the scene it runs the dashboard from workbooks
- Diagnostic settings: detailed logs created by the resources. You can send those logs to log-analytics, or store the logs etc.
- Autoscale: increase/decrease capacity of resource
- Usage and Costs: quick view about costs
- Advisor reccomandation: personalised best practice to improve performance, costs, security etc.
- New Support Request

### Storage Account Monitoring

In storage account there is a section called: `Insights`.
It allows you to see charts based on metrics:
- Overview: main charts about Availability, Transaction, E2E latecy and server latency.
- Failures charts.
- Performance charts: where you can see the API that was causing most of the latency, if latency was more on ops or more on network.
- Capacity.

In `Metric` section. You can select from a big number of metrics and see the chart of that specific metric (eg. ingress, egress) charts.

In `Diagnostic settings`. You can on and off the service. You have hours metrics, minute metrics, and logging. You can decide
which metric you want to store and how want we want to retain these data. To access the log container we have to go to
storage explorer. In the blob container you have a container named `$logs`, while for the metrics you have to go in `Tables` and the table have `$Metrics` in the name.

### Synapse Monitoring

You have a section called `Monitoring`.
- `Query activity`: you can see the logs for every query. You can see the statement, the duration, the status etc.
- `Alerts`: you have to create a rule, selecting the resource, condition and actions. And then you can see the alerts triggered.
- `Metrics`: you can choose the scope, the metrics, and you can see a chart.
- `Diagnostic settings`: store metrics

### CosmosDB Monitoring

- `Metrics`: you have different tabs, with different aggregated views of metrics. In the main tab you can see the main charts from all those tabs: average throughput per seconds, data size, index size. Number of requests divided per type (429, requests failed because exceeded the RU). Important are also *latency* and *consistency* among data in different regions.
- `Alerts`: set alerts based on metrics
- `Diagnostic settings`: store set of metrics and logs in the storage account or send to EH etc.
- `Logs`: you need to enable Diagnostic settings to see and perform queries on collected logs

### Data Factory monitoring

We want to monitor the pipelines when we are in PRD: error and resource consumed.
On Azure Data Factory itself we can go in the `Monitor` section. On Monitor section
you have 5 sections:

- Dashboard: have a quick view of your run, successful runs, activities, triggers.
- Pipeline runs: you can filter the pipeline by status, you can look to the error etc, duration of pipeline, resource consumpt, etc. You can see piepeline parameters, filter by notations etc. You have a GANTT view where you have a more graphical way. Clicking on the eye-glass you can see more detail of the pipeline, number of DU consumed etc, numer of rows processed.
- Trigger runs: what are the triggers that run.
- Integration rumtime: you can filter based on the integration runtime, you can see the pipelines that run by the IR.
- Alert and metrics: if you click on alert, you can create an alert. If you click on Monitor, you go in another page:
    For metrics, you have charts based on the some metric that you select (eg. Failed activity, count, timeframe)
    Alerts
    Diagnostic settings

### Databricks monitoring

If you click on Cluster you have: 
- Ganglia UI where you can see charts of hardware status of the cluster.
- Spark UI
- Driver/Cluster Log4j, stdout, stderr
- Driver event log

You can send the log4j logs to log analytics using external libraries.
You can connect databricks with *Grafana*, Prometheus, and other services.

### Stream Analytics monitoring

We can monitor Portal, or use Azure SDK and API.
Once inside the Streaming Anlaytics Job you have the all the common monitoring tools:
- Metrics: important metrics are SU metrics, runtime errors, Watermark Delay (difference between processed time and even-time, normal 6 seconds), Input Deserialization errors.
- Alert rules
- Diagnostic logs
- Logs

## Optimize Data Solution

### Data partitioning

Process of physically dividing the data into separate data store.
There are 3 strategies for partitions:
- Horizontally: 1000 rows. 500 rows in one partition, 500 rows in another partitions.
- Vertically: 1000 rows. Columns divided into two tables, still of 1000 rows each.
- Function: divide data based on a function

Best practice:
- Balance data distribution
- Balance workload distribution
- Minimize joins accross partitions
- Evaluate the consistency

### Optimize streaming analytics.
SUs (Streaming Units), hardware allocated to your streaming jobs.
If SU utilization, input events get backlog.
If it is close to 80% we need to increase SU or improve the query performance.
Start from assigning 6 SU to the query.

Partitioning helps to divide data in subset. Partitions are based on a partition_key.
If the data in the event hub has a partition key defined, it is highly reccomanded to define it
as partition key for the Streaming job. If the input is partitioned, output needs to be partitioned.
Best case is connect one partition of the input with one partition to the output.

```sql
SELECT *
INTO output
FROM input
PARTITION BY DeviceID
INTO 10
```

### Azure Synapse Analytics

- Enable `AUTO_CREATE_STATISTICS` property. It can help for the queries on more relevant columns (like data).
- Use `Polybase` for large volume of data.
- Hash distributing of large table. By default tables are distributed in round-robin. But for big tables, using hash distribution will be very fast.
- Do not over partition. 
- Use smallest possible column size.
- Scale out before perform heavy data loading or during peak business hours.

### Managing data Lifecycle

We are talking about Storage account: there are access tiers: cool, hot, archive.
We can apply some rules to move data between the different tiers. For example, we can set a rule so that we move a blob
after 90 days from the latest modification from hot to cool. The policy can be set at container level.

In the storage account you can setup the default access tier: cool or hot. This is at account level.

To set a new rule: open the storage account instance. In the menu, under the section `Blob Service` open `Lifecycle Managment` to set a new rule.
