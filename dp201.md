# DP-201

## Reccomanded data solution per requirements

### Data Types

- Structure: highly organized, with schema, relation database, SQL. We have to define the schema even before of writing. **schema-on-write**
- Semi-structure: JSON, CSV etc. Schema is not strictly enforced. **schema-on-read**.
- Unstructured: data without pre-defined data model, no structure.
- Streaming: data in flow, reacting in real-time.

### Relational Database

Relational database.
Define schema before.
Highly normalised.
SQL language.
ACID properties (CONSISTENT!) - perfect for order managments.

You have SQL Database, MySQL, MariaDB and so on.

### Non relational database

Key/value store.

Optimized for look-up: session, cache.
Redis.
CosmosDB.
Storage account.

### Document database

Similar to key value.
But the value has a document JSON (semi-structured).
Structure is not fixed.

Operations:
- Product catalog.
- Inventory managment.

Services:
- CosmosDB

### Graph Database

GraphDB with gremlin API.
Efficiency perform queries when you have a lot of relationships (friends).

### Column-family

We combine multiple column in one single column, we expect to retrieve together.

We use in DWH.

Service:
- Cassandra API.

### Data Analytics

Working in parallel and distribute computing into different nodes.
DWH or Synapse SQL Analytics (MPP), Data Lake we can use to store big objects.

### Object Storage

Optimized for storing large binary objects.
Azre blob storage, Gen2.

### Shared files.
Requires SMB interface.
Azure files.

### Time series database

Data element that have tag with the time. Used for IoT mostly.
Service: Azure Time Series Insights.

### Azure Search Engine

When working with big data.
Create a catalog with which data can be find where.

## Non relational solutions

### CosmosDB partitioning

When we define the container we define the partitions key.
An entire logically partition goes into one physically partitions.
If logical partitions are small, more logical partitions can go inside the same physical partitions. One logical partitions can't be split into multiple physical partitions. It is also possible that ComosDB change the physical location of the partitions, but it changes it entirely.

### Thoughput, Partition

Thoughput = request that cosmosdb can handle (RU/s).

Either at database level or at container level.
Defining the throughtput in container level you have dedicated reserved throughput for the same container. You can also mix container level throughput and database level.

When you have throughtput at container level, it equally divides in all the partitions. That means, you always want to use all the partitions equally. You do not want underutilized partitions. You should choose the partition key to do that.

Good candidate: user_id.
Bad candidate: timestamp.

You can query just within the same container.
You prefer query that filter by partitions, so it reads just the data of the partition.

There are two restrictions:
- Each document can't exceed 2MB -> you can devide the document and have an index document.
- Each partition can't exceed 20GB -> you can maybe use a composite-key, to have more granular partitions `customer_name-year`.

### Globally distributed database

From the portal, if you go to `Replicate Data Globally` you can enable/disable the distribution to another region. You can choose if you want to enable those other location as read-only location or read-write location.
It makes sense to pick the paired regions to have more speed if the purpose is just backup. Otherwise pick the region close to your customer.

In case of read-write location, there could be conflicts. You can decide from the portal how to solve those conflicts, from the timestamp or from specific rules.

#### In case of disaster recovery

If one read-only datacenter goes down. User is redirected to the next closest regions. It does this automatically.

If the in write region fails, if you don't have other write region. You can go to do a manual fail-over to choose the region to promote, otherwise you can set the candidate and enable the automatic-failover.

### Consistency level

- Strong: all replicas sync at the same time.
- Bounded Stateness: Strong but with acceptable delay.
- Session: you can read, not necessary the latest data. Eventually you will receive all the updates (order is guaranted). Within the session you will see the latest data.
- Consistent prefix: you can read, not necessary the latest data. Eventually you will receive all the updates (order is guaranted).
- Eventual: you can read, not necessary the latest data. Eventually you will receive all the updates (order is not guaranted).

You set the consistency level you want per account.
You can change consistency level per request.


### Multi model

- Cosmos SQL
- MongoDB (only choose if you are migrating)
- Table API
- Cassandra
- Gremlin

## High Availability

High Availability: should remain available, if one instance goes down, the other instance takes its place (within the same region).

Disaster Recovery: from site level events. We recover from different region.

RTO: Recovery Time Objective. Accepted time that we could be down.
RPO: Recovery Point Objective. Accepted data loss.

In case of Disaster recovery, you first are going to lose some data (RPO) and then you can decide to take your service goes down (RTO).

### Storage account

- LRS: 3 copies in different racks in the same zone in G1
- ZRS: 3 copies in 2 or 3 availability zones in G1
- GRS: (3 copies in 1R) and (3 copies in 2R). You can read from G2 just in case of fail-over.
- GZRS: (3 copies in different availability zones in G1) and (3 copies in G2). You can read from G2 just in case of fail-over.
- RA-GRS: like GRS but you can have access to G2 copies even without fail-over.
- RA-GZRS: like GZRS but you can have access to G2 copies even 


### CosmosDB and Backups

In certain region, you can enable the Availability Zones.
If the entire region goes down, we use the global distribution of CosmosDB.

Backup: only two copies, stored in the same region and in paired region. You can decide the retention time, and the interval.
To recover you have to open a support request.


## Design Relational Database

### SQL Server

Different types of SQL Server:

- Single Database
- Elastic Pools
- Managed Instance (a lot of flexibility but still SaaS, ideal for migration)

#### Pricing Tiers

Provisined: you select a given set of resources.
Serveless: it automatically gets more resources.

Two pricing tiers:

- DTU (CPU+Memory+IO): not for managed instances
    - Basic
    - Standard
    - Premium
- vCore (You can adjust individual resources)
    - General Purpose (equivalent to Standard 99.99)
    - Business critical (equivalent to Premium storage is faster. 99.99)
    - Hyperscale: intended for large database 200TB

You can convert between the two pricing tiers.

#### Scaling

We have vertical (in terms of DTU and vCORS, there is no auto-scaling) and horizontal scaling (*copy into read only instances* or *divide the data into different databases*).

Dynamic scalability is different from autoscale. 
Autoscale is when a service scales automatically based on criteria, whereas dynamic scalability allows for manual scaling with a minimal downtime. Single databases in Azure SQL Database support manual dynamic scalability, but not autoscale. 

#### HA and fail-over

#### Inside the same region

Standard:
    Compute and Log files separated.
    If the node goes down, as part as fail-over, there could be potential degradation in performance, mainly because of the cache.
    Data stored in LRS.
    Backup stored in RA-GRS.
Premium:
    Compute node has
    3 replicas. Cache is replicated too in at least 1 node. In case of fail-over, no performance impact.
    Since secondary node has exactly the same data, including cache. You can use it also to read-only (no extra cost).
    You also have Zone Replication.
    Backup files in RA-GRS.

#### Geo replication

You can enable the geo-replication, defining another server in another region.

#### Fail over groups

Issues;
- Endpoint must be changed.
- No automatic fail-over.

Solution:
- Create Fail over group. You can define automatic fail-over after some grace period. You have read/write end-points you can set.

#### Backup

- Full backup: taken every week (includes the transaction logs)
- Differential backup: differential backup from a full backup. taken every 12 hours
- Transaction log backup: every 10 minutes.

You should use a combination of a full weekly backup, a differential backup, and
transaction log backups. A full backup backs up the entire database. This type of
backup includes the transaction log so that the entire database can be restored if the
full backup is restored. A differential backup backs up data since the most recent full
backup. It captures only the data that has changed. Therefore, they are a lot smaller
than full backups. A transaction log backup backs up the transaction logs. A
transaction log consists of all the transactions that have occurred. This solution uses
the least amount of disk space because only one full backup occurs every week, and
differential backups use only enough disk space to capture changes since the last full
backup. By first restoring the previous full weekly backup, you restore the database to
the state it was in the previous week. By next restoring the differential backup from
the last 12 hours, you restore all the changes that occurred between last week and the
last 12 hours. By finally restoring the transaction log since the last differential backup,
you restore the database to the point of failure. These backups are supported by
automatic backups.

Backup Retention period: 7-35 days. (max 7 days for Basic Tier)
Long-retention: up to 10 years.

Backup does not work if you delete the Azure Logical SQL.

When you have to do a backup. Load the most recent full backup first.
Then the most recent differential backup and then the most recent logs.

### DWH

You can choose the sharding pattern:
- Hash: distribute data based on a column (good for large tables, facts)
- Round-robin: distribute evenly the data in each compute node (good for loading data and for staging tables).
- Replicated: cache full copy of the table in each compute node

Avoid Data Skew: when all the vast majority of the data is in one distribution. Ideal scenario: even distribution.

Data types -> we want to use the smallest data type.

Table types:
- Clustered columnstore: meant for large tables, organize records by column.
- Heap: no index on data, no ordering on the data. No compression.
- Clustered B-Tree Index: organized by a sorted column.
 
Table partitioning: usually data are partitions by date.
Why:
- Easy to load/remove data
- Improve query performance

You don't want to have too many table with too many partitions (1-100).

#### Scaling

DWU (CPU/Memory/IO) that we can increase or decrease.

#### HA - Backup - Geo replication

Azure Synapse Analytics uses database snapshots to provide high availability of the warehouse.
Snapshots, Retention period 7 days.
We can also have user-defined snapshot, for 7 days. maximum 42 restore points.
Replicate to pair region, once in a day. (enabled by default)

Not really an HA because restoring from backup takes time and it is a manual process.

## Design batch processing solution

Storage:
- Blob
- Data Lake (prefered)

Batch processing:
- U-SQL (Azure Data Lake Analytics)
- Spark (Databricks)
- HDInsight

Orchestration:
- Data Factory
- Oozie
- Basic orchestration in Databricks

Data Ingestion:
- PolyBase to Synapse Service (DWH)
- Put data inside storage, using storage explorer, azure cli, data factory.

## Design real-time processing solutions

Ingestion:
- Event Hub, native support AMQP
- IoT hub
- Kafka: open message queing.
- Blob Storage, Data Lake

Stream Processing:
- Azure Stream Analytics
    Fully managed, stream analytics.
    Process data in real-time. Language is T-SQL.
    You need to choose the Streaming Unit.
    It can get streaming input from EH, IoT Hub, Store Blob (check if new file is created)
    It can get reference input Blob Storage, SQL.
    It delivers to a lot of service: power-bi, event-hub, storage.
    You should partitions input input and output.
    Different windows:
        - Tumbling window: every 10 minutes, no overlap. fixed length.
        - Hopping: Every 5 seconds, create a window that last 10 seconds. can overlap. fixed lenght.
        - Sliding: every new event create a window a look behind of 10 seconds. can be overlap. fixed length.
        - Session window: window open when event arrives, if finish if no events comes since X second or it finish anyway after X. No overlap. No fixed window size.
- Databricks

Pattern:
From EH -> Stream Analytics -> Dashboard. (No Storage needed).
From EH -> Stream Analytics -> SQL Database -> Dashboard. (Dashboard can also visualise past data, higher latency).
From EH -> Stream Analytics -> EH/Serverless. (Good for alertings).
From EH -> Stream Analytics -> SQL Database -> App Service.
From EH -> Stream Analytics + Reference Data -> SQL Database -> X.

Lambda Architecture:
There are business requirement where we want to process data both in batch and streaming.


## Security

### Connect your Virtual Network to Service

All Azure resources have public end-point. Everyone can access the resource.
You can limit the access to resource to specific Virtual Network and allow traffic from a specific IP (firewall).
You can link then the VM inside the Virtual Network using a `Service Endpoint` and you can't restrict to a particular storage account, for example. You just enabling the connection to the service from the virtual network using its own public endpoint. Instead, you can use `Private Endpoint` to restrict to a given storage account instead. Instead of accessing the public endpoint, you can access the service from a particular IP inside your VM. Now you can use that IP for different policies.

### Storage accounts

You can access using Access Keys (2 access keys).
It is better to access using SAS keys, signed using access keys, can restrict the access of SAS key by resources, permission or time.

Service Shared access signature (SAS): give access to a single storage service (blob, but not table for example).
Account Shared access signature (SAS): give access to all the storage service (blob and table for example).

SAS Key can be signed by AD identity. That takes the name of `User Delegation Shared Key`, and it is recommanded when possible.

You can't access to File Share using SAS.

Key that works for all the service is still the Shared Key (Primary or Secondary).

### Azure Activity Directory

Microsoft has introduce AAD identities (user or service principle).
You can assign role to a AAD identiy. You can grant access to a specific container, or folder using AAD.
Now, no need or credentials and keys.

#### RBAC

Used to deletage resource administration to AAD identies or groups.
For example, you can assign the role of "Contributor" to a specific user or identities.
RBAC are inherit. If I apply the RBAC to subscription, and it is inherit also on all the resources inside there.

## Encryption for data

- Data encrypted in Rest, all the service have encryption at rest by default. Sometimes you can't disable (blob storage). You can just change the key to have a managed key or your own key that you can put in key-vault. On SQL/DWH you can disable Data Encryption. You can choose the key at the server-level, not at the database. On ComosdDB there is no encryption option, always encrypted.
- SQL Server and DWH have the `transparent-data-encryption` (data encrypted at rest) by default.
- For SQL Server and DWH, you can additionally choose encrypt colum using CEK, other than transparent data encryption. It is called `Always Encryption` and the data are encrypted server side. You need to define a master-key for it. This is an encryption that is applied just for column. Protect data `in-use` from users (including admins) that does not need to know. 
- Data encrypted in Motion
    - Use SSL
    - Use site-site VPN or point-to-site VPN (site to site VPN means a VPN that has a connection always open, good for application, while point-to-site is a connection that you enable when you need, like VPN for connection to the office, good for developers)
    - Express Route is just private but it is not encrypted. But you can encrypt your data that pass though the express route.
- Data in use (generated, updated...)

## Data Auditing

You can enable auditing at server or database level.
You can write logs to storage, log analytics etc.

## Data Masking

You can set the masking function from the portal: Sql Database. Dynamic Data Masking.
Admin can see everything.
You have to set an user and assign a to a role.
Masking is not encryption. User can fuzz and guess the data using the query.

- Random Number Function (works just for numbers)
- Email function (aXXXX@XXXXX.com)
- Custom string function (specify the characters)
- Default function (XXXX everything)
- Credit card function (xxxx-xxxx-xxxx-1234)

## Data Privacy and Classification

For SQL and DWH:

**Advance Data Security** allows you to
    - identifies and label sensitive data.
    - discover vulnerabilities and improve security issues
    - detects anomalies activities

## Data Retention and Archiving

Hot
Cool
Archive

Policy to delete, move between tiers based on the last access.